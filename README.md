<div align="center">

<h1>APB: Accelerating Distributed Long-Context Inference by Passing Compressed Context Blocks across GPUs</h1>

<p align="center">
<a href="https://huangyuxiang03.github.io/blogs_apb" target="_blank">Blog</a> |
<a href="" target="_blank">Paper (ArXiV)</a> 
</a>
 
</p>

**10x Lossless Long-Context Inference Speedup with Sequence Parallelism-Aware Approximate Attention**
</div>

APB is a distributed long-context inference framework that leverages multi-host approximate attention to enhance inference speed, achieving speedups of up to **9.2x**, **4.2x**, and **1.6x** compared to **FlashAttn**, **RingAttn**, and **StarAttn**, respectively.

##
<div align="center">
<h4>This project was made possible thanks to a collaboration with <img src="figures/univ.png" height="60px" align="center"/></h4>
</div>

##



## Design


![](figures/design.png)



## Usage

TODO



## Citation

Please cite our [paper](https://arxiv.org/abs/xxxx) if you find our work valuable.

```
@article{huang2025apb,
  title={APB: Accelerating Distributed Long-Context Inference by Passing Compressed Context Blocks across GPUs},
  author={Yuxiang Huang, Mingye Li, Xu Han, Chaojun Xiao, Weilin Zhao, Sun Ao, Jie Zhou, Hao Zhou, Zhiyuan Liu, Maosong Sun},
  journal={arXiv preprint arXiv:xxxx},
  year={2025}
}
```

